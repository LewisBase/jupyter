{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用机器学习与情感分析\n",
    "\n",
    "本章将主要涵盖下述几个方面\n",
    "\n",
    "* 清洗和准备文本数据\n",
    "* 根据文本数据建立特征向量\n",
    "* 训练机器学习模型来区分正面或者负面评论\n",
    "* 用基于外存的学习方法来处理大型文本数据集\n",
    "* 根据文档推断主题进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为文本处理预备好IMDb电影评论数据\n",
    "\n",
    "### 获取电影评论数据集\n",
    "\n",
    "### 把电影评论数据预处理成更方便格式的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:09\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# change the 'basepath' to the directory of the\n",
    "# unzipped movie dataset\n",
    "\n",
    "basepath = 'aclImdb'\n",
    "\n",
    "labels = {'pos':1,'neg':0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test','train'):\n",
    "    for l in ('pos','neg'):\n",
    "        path = os.path.join(basepath,s,l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path,file),\n",
    "                     'r',encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt,labels[l]]],\n",
    "                           ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review','sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码首先初始化新的进度条对象pbar，并定义迭代次数为50000，这是要读入的文件数量。使用嵌套的for循环，遍历aclImbd主目录下的train和test子目录，并从子目录pos和neg下读入单个文本文件，这两个目录连同整数类标签（1=正面和0=负面）最终将会被映射到pandas的DataFrame对象df上。\n",
    "\n",
    "因为数据集中的分类标签已经排过序，所以现在可以调用np.random子模块的permutation函数对DataFrame洗牌，这对后期将数据集分裂成训练集和测试集很有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>In the New Year's Eve, the tuberculous sister ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Mr. Kennedy should stop ExPeRiMeNtIng with bad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>This is a terrible production of Bartleby, tho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In the New Year's Eve, the tuberculous sister ...          1\n",
       "1  Mr. Kennedy should stop ExPeRiMeNtIng with bad...          0\n",
       "2  This is a terrible production of Bartleby, tho...          0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv',encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词袋模型介绍\n",
    "\n",
    "词袋模型背后的逻辑非常简单，可以概括如下：\n",
    "\n",
    "1. 从整个文档中创建一个基于独立令牌（例如单词）的词汇表。\n",
    "2. 为每个文档构建一个特征向量，其中包含每个词在特定文档中出现的频率。\n",
    "\n",
    "由于么个文档的独立单词只代表了词袋词汇表中所有单词的一小部分，所以特征向量主要由零组成，因此称之为稀疏向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把此转换成特征向量\n",
    "\n",
    "可以用scikit-learn实现的CountVectorizer类根据单词在各文件中出现的频率构建词袋模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'\n",
    "])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用CountVectorizer的fit_transform方法处理构建词袋模型的词汇表，并且把以下三个句子转化为稀疏特征向量。\n",
    "\n",
    "现在打印出词汇表的内容，以便更好地理解其中所包含地概念"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n",
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征向量的每个索引位置对应于词汇存储在CountVectorizer字典项中的整数值。例如，索引位置0上的第一个特征等同于单词'and'的词频，它只出现在最后一个文档中，单词'is'在索引位置1（文档向量中的第二个特征），它出现在所有三个句子中。这些值的特征向量也叫__原词频率__:tf(t,d)，即t项在文档中出现的次数d。\n",
    "\n",
    "> 刚创建的词袋模型也被称为__1克__或者__单克__模型，词汇中的每项或每个令牌代表一个单词。更普遍的是NLP中的连续序列项，即单词、字母或者符号，也被称为__n克__。在n克模型中所选择的数量n取决于特定应用。例如，坎纳瑞斯和其他人的研究发现，在反垃圾邮件过滤过程中，n克模型的规模设置为3和4时性能最佳。总结n克的概念，表示第一个文件\"the sun is shining\"的1克和2克模型的构成如下：\n",
    "> * 1克：\"the\",\"sun\",\"is\",\"shining\"\n",
    "> * 2克：\"the sun\",\"sun is\",\"is shining\"\n",
    "\n",
    "> scikit的CountVectorizer类通过调整参数ngram_range来使用不同的n克模型。默认情况为1克模型，可以通过转化一个新的CountVectorizer实例，同时通过定义ngram_range=(2,2)将其转换为2克模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过词频逆反文档频率评估单词相关性\n",
    "\n",
    "__词频逆反文档频率(td-idf)__，用于减少特征向量中频繁出现地词。tf-dif可以定义为词频与逆反文档频率地乘积：\n",
    "\n",
    "$$tf-idf(t,d)=tf(t,d)\\times idf(t,d)$$\n",
    "\n",
    "tf(t,d)为前一节引入的词频，idf(t,d)为逆反文档频率，起计算过程如下：\n",
    "\n",
    "$$idf(t,d)=\\log \\frac{n_d}{1+df(d,t)}$$\n",
    "\n",
    "$n_d$为文档总数，$df(d,t)$为含有单词t的文档数量。请注意，为分母添加常数1为可选，目的在于所有训练样本中出现地单词赋予非零值，用对于来确保低文档频率地权重不会过大。\n",
    "\n",
    "scikit-learn实现了另外一个转换器TfidfTransformer类，它以来自于CountVectorizer类，以原始词频为输入，然后转换为tf-idfs格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True,\n",
    "                         norm='l2',\n",
    "                         smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs))\n",
    "      .toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如前一节所看到的，单词'is'在第三个文档中的词频最高，是最常出现的单词。然而，在把相同的特征向量转换成tf-idf后，发现单词'is'在第三个文档中与相对较小的tf-idf(0.45)相关联，因为该页也在第一和第二个文档中出现，因此不太可能包含任何具有判断性的信息。\n",
    "\n",
    "然而，如果手工计算特征向量中的每个单词的tf-idfs，就会注意到TfidfTransformer对tf-idf的计算与之前书中定义的标准公式稍有不同。scikit-learn实现的逆文档频率计算公式如下：\n",
    "$$idf(t,d)=\\log\\frac{1+n_d}{1+df(d,t)}$$\n",
    "类似，在scikit-learn中计算的tf-idf与前面定义的默认公式稍有不同：\n",
    "$$tf-idf(t,d)=tf(t,d)\\times (idf(t,d)+1)$$\n",
    "\n",
    "在调用TfidfTransformer类直接归一化并计算tf-idf之前，归一化原始单词的词频更具有代表性。定义默认参数norm='l2'，用scikit-learn的TfidTransformer进行L2归一化，返回长度为1的向量，\n",
    "$$v_{norm}=\\frac{v}{\\|v\\|_2}=\\frac{v}{\\sqrt{v_1^2+v_2^2+\\dots+v_n^2}}=\\frac{v}{(\\sum_{i=1}^n(v_i^2)^{\\frac{1}{2}}}$$\n",
    "\n",
    "为了确保理解TdifTramsformer的工作机制，让我们来看下面三个文件中如何计算单词'is'的tf-idf。\n",
    "\n",
    "单词'is'在第三个文档中的词频为3(tf=3)，其文档频率也为3因为单词'is'在所有的三个文档中都出现过(df=3)。因此可以计算逆文档频率如下：\n",
    "$$idf(\"is\",d3)=\\log\\frac{1+3}{1+3}=0$$\n",
    "现在，为了计算tf-idf，只需要在逆文档频率上加1，然后乘以词频\n",
    "$$tf-idf(\"is\",d3)=3\\times(0+1)=3$$\n",
    "如果重复对第三个文档中所有术语的计算，将获得tf-idf向量：[3.39,3.0,3.39,1.29,1.29,1.29,2.0,1.69,1.29]。然而，请注意，这个特征向量中的值与以前用TfidfTransformer所获得的不同。在tf-idf计算中缺少的最后一步是L2归一化：\n",
    "$$tf-idf(d3)_{norm}=\\frac{[3.39,3.0,3.39,1.29,1.29,1.29,2.0,1.69,1.29]}{\\sqrt{[3.39^2+3.0^2+3.39^2+1.29^2+1.29^2+1.29^2+2.0^2+1.69^2+1.29^2}}$$\n",
    "$$tf-idf(\"is\",d3)=0.45$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清洗文本数据\n",
    "\n",
    "在构建词袋模型之前，第一个重要的步骤是通过去掉所有不需要的字符来清洗文本数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'razil): \"A Carroça Fantasma\" (\"The Phantom Coach\")'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0,'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the New Year\\'s Eve, the tuberculous sister of the Salvation Army Edit (Astrid Holm) asks her mother and her colleague Maria (Lisa Lundholm) to call David Holm (Victor Sjöström) to visit her in her deathbed. Meanwhile, the alcoholic David is telling to two other drunkards in the cemetery the legend of the Phantom Coach and his coachman: in accordance with the legend, the last sinner to die in the turn of the New Year becomes the soul collector, gathering souls in his coach. When David denies to visit Edit, his friends have an argument with him, they fight and David dies. When the coachman arrives, he recognizes his friend Georges (Tore Svennberg), who died in the end of the last year. George revisits parts of David\\'s obnoxious life and in flashbacks, he shows how mean and selfish David was.<br /><br />\"Körkarlen\" is an impressive and stylish silent movie, with magnificent special effects (for a 1921 movie). The characters are very well developed; however, the story is dated and there is a weird and unexplained situation, when Sister Edit tells that she loves David Holm. Why should a enlightened woman love such a despicable man that wasted his life corrupting other people? Despite being religiously dated in the present days, it gives a beautiful message of faith and redemption in the end. My vote is nine.<br /><br />Title (Brazil): \"A Carroça Fantasma\" (\"The Phantom Coach\")'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0,'review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如这里看到的，文本中包含HTML标记和标点符号以及其他的非字母字符。虽然HTML标记所包含的有用语义不多，但在某些NLP场景，标点符号可以包含有用的附加信息。然而，为了简单起见，将删除所有的标点符号除了表情特征如:)，因为这些符号当然是有用的情感分析数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>','',text)\n",
    "    emoticons = re.findall('(?::|:|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text)\n",
    "    text = (re.sub('[\\W]+',' ',text.lower()) +\n",
    "            ' '.join(emoticons).replace('-',''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一轮用正则表达式<[^>]\\*>试图删除电影评论数据集中的所有HTML标记。之后用稍微复杂的正则表达式来找到表情符号，并暂存为emoticons。最后通过正则表达式[\\w]+去除所有的非单词字符并把文本转换为小写字符。\n",
    "\n",
    "尽管把表情字符加在清洗干净的文档字符串的结尾看起来可能不是最优雅的方法，当应当注意如果词汇中只包含单词令牌，词袋模型中词语的顺序就无关紧要了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teisninetitlebrazilacarroçafantasmathephantomcoach'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[0,'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor) #数据清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把文档处理为令牌\n",
    "\n",
    "标记文件的一种方法是通过把清洗后的文件以空白字符拆分为单词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于文档的表计划，还有另外一种有用的技术是__词干__，就是将单词转换为词根的过程。可以把相关的词映射到同一个词干上。原始算法是马丁·波特1979年开发的__波特分词__算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用NLTK软件包的PorterStemmer函数，修改tokenizer函数把相关的词都归纳为相应的词根。\n",
    "\n",
    "__停用词删除__，去除停用词可能对处理原始或者正则词频而非tf-idf有益，因为tf-idf已降低了频繁出现的单词的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/linux1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练文档分类的逻辑回归模型\n",
    "\n",
    "首先将清理过的文本文档分成训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000,'review'].values\n",
    "y_train = df.loc[:25000,'sentiment'].values\n",
    "X_test = df.loc[25000:,'review'].values\n",
    "y_test = df.loc[25000:,'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着用GridSearchCV对象，采用5倍分层交叉验证方法，为逻辑回归模型寻找最佳参数集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "/home/linux1/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   47.6s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=2)]: Done 240 out of 240 | elapsed:  4.3min finished\n",
      "/home/linux1/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=False,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_word...\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x14d3b8b420e0>,\n",
       "                                              <function tokenizer_porter at 0x14d3b8b42290>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range':[(1,1)],\n",
    "               'vect__stop_words':[stop,None],\n",
    "               'vect__tokenizer':[tokenizer,\n",
    "                                  tokenizer_porter],\n",
    "               'clf__penalty':['l1','l2'],\n",
    "               'clf__C':[1.0,10.0,100.0]},\n",
    "              {'vect__ngram_range':[(1,1)],\n",
    "               'vect__stop_words':[stop,None],\n",
    "               'vect__tokenizer':[tokenizer,\n",
    "                                 tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty':['l1','l2'],\n",
    "               'clf__C':[1.0,10.0,100.0]}\n",
    "             ]\n",
    "lr_tfdif = Pipeline([('vect',tfidf),\n",
    "                     ('clf',\n",
    "                      LogisticRegression(random_state=0))])\n",
    "gs_lr_tfdif = GridSearchCV(lr_tfdif,param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,verbose=1,\n",
    "                           n_jobs=2)\n",
    "gs_lr_tfdif.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网格搜索完成后，可以显示得到的最佳参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], 'vect__tokenizer': <function tokenizer at 0x14d3b8b420e0>}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s'%gs_lr_tfdif.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy:0.506\n"
     ]
    }
   ],
   "source": [
    "print('CV Accuracy:%.3f'\n",
    "      % gs_lr_tfdif.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.506\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfdif.best_estimator_\n",
    "print('Test Accuracy: %.3f'\n",
    "      % clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理更大的数据集——在线算法和核心学习\n",
    "\n",
    "__核外学习__，通过对数据集的小批增量来模拟分类器完成大型数据集的处理工作。\n",
    "\n",
    "本节将用scikit-learn的SGDClassifier的partial_fit函数从本地驱动器直接获取流式文件，并用文件的小批次文档训练逻辑回归模型。\n",
    "\n",
    "首先定义tokenizer函数来清理来自于movie_data.csv文件的文本数据，然后分解成单词，在标记的同时去除了停用词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>','',text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+',' ',text.lower()) \\\n",
    "            + ' '.join(emoticons).replace('-','')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r',encoding='utf-8') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3],int(line[-2])\n",
    "            yield text,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In the New Year\\'s Eve, the tuberculous sister of the Salvation Army Edit (Astrid Holm) asks her mother and her colleague Maria (Lisa Lundholm) to call David Holm (Victor Sjöström) to visit her in her deathbed. Meanwhile, the alcoholic David is telling to two other drunkards in the cemetery the legend of the Phantom Coach and his coachman: in accordance with the legend, the last sinner to die in the turn of the New Year becomes the soul collector, gathering souls in his coach. When David denies to visit Edit, his friends have an argument with him, they fight and David dies. When the coachman arrives, he recognizes his friend Georges (Tore Svennberg), who died in the end of the last year. George revisits parts of David\\'s obnoxious life and in flashbacks, he shows how mean and selfish David was.<br /><br />\"\"Körkarlen\"\" is an impressive and stylish silent movie, with magnificent special effects (for a 1921 movie). The characters are very well developed; however, the story is dated and there is a weird and unexplained situation, when Sister Edit tells that she loves David Holm. Why should a enlightened woman love such a despicable man that wasted his life corrupting other people? Despite being religiously dated in the present days, it gives a beautiful message of faith and redemption in the end. My vote is nine.<br /><br />Title (Brazil): \"\"A Carroça Fantasma\"\" (\"\"The Phantom Coach\"\")\"',\n",
       " 1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='movie_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在将定义get_minibatch函数，该函数调用stream_docs读入文件流并返回大小由参数size定义的文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream,size):\n",
    "    docs,y = [],[]\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn实现的另一个有用的向量化工具是HashingVectorizer，该工具用于文本处理并独立与数据，基于哈希技术。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log',random_state=1,max_iter=1)\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过标记化函数并设置特征数量为2\\*\\*21来初始化HashingVectorizer。另外，设置loss参数SGDClassifier的值为'log'，重新初始化逻辑回归分类器，请注意如果在HashingVectorizer中选择较大的特征数，可以减少哈希碰撞的机会，但是也增加了逻辑回归模型系数的个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:18\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0,1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream,size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train,y_train,classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.868\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream,size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具有潜在狄氏分配的主题建模\n",
    "\n",
    "主题建模描述了为无标签文本文档分配主题这个范围很广的任务。本节主要介绍__潜在狄氏分配（LDA）__。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用LDA分解文本文档\n",
    "\n",
    "LDA是一种概率生成模型，试图找出经常出现在不同文档中的单词。假设每个文档都是由不同单词组成的混合体，那么经常出现的单词就代表主题。\n",
    "\n",
    "LDA将把词袋矩阵作为输入然后分解成两个新矩阵：\n",
    "* 文档主题矩阵\n",
    "* 单词主题矩阵\n",
    "\n",
    "LDA的缺点是必须预先定义好主题数量这个超参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA与scikit-learn\n",
    "\n",
    "本节将用scikit-learn的LatentDirichletAllocation类来分解电影评论数据集，然后归入不同的主题。数量限制在10个以内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('movie_data.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着将用CountVectorizer创建词袋矩阵作为LDA的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意把要考虑单词的最大文档频率设置为10%(max_df=.1)，以排除在文档间频繁出现的那些单词。\n",
    "\n",
    "把考虑单词的数量限制为5000(max_feature=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1\n",
      "worst minutes awful script stupid\n",
      "Topic 2\n",
      "family mother father children girl\n",
      "Topic 3\n",
      "american dvd music tv war\n",
      "Topic 4\n",
      "human audience cinema art sense\n",
      "Topic 5\n",
      "police guy car dead murder\n",
      "Topic 6\n",
      "horror house sex gore blood\n",
      "Topic 7\n",
      "role performance comedy actor performances\n",
      "Topic 8\n",
      "series war episode episodes season\n",
      "Topic 9\n",
      "book version original effects read\n",
      "Topic 10\n",
      "action fight guy guys cool\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort() \\\n",
    "                       [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horroe movie #1:\n",
      "Emilio Miraglia's first Giallo feature, The Night Evelyn Came Out of the Grave, was a great combination of Giallo and Gothic horror - and this second film is even better! We've got more of the Giallo side of the equation this time around, although Miraglia doesn't lose the Gothic horror stylings tha ...\n",
      "\n",
      "Horroe movie #2:\n",
      "House of Dracula works from the same basic premise as House of Frankenstein from the year before; namely that Universal's three most famous monsters; Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie together. Naturally, the film is rather messy therefore, but the fact that ...\n",
      "\n",
      "Horroe movie #3:\n",
      "This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ...\n"
     ]
    }
   ],
   "source": [
    "horror = X_topics[:,5].argsort()[::-1]\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print('\\nHorroe movie #%d:' % (iter_idx + 1))\n",
    "    print(df['review'][movie_idx][:300],'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.36, 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       [0.68, 0.  , 0.  , ..., 0.  , 0.  , 0.06],\n",
       "       [0.07, 0.  , 0.  , ..., 0.  , 0.65, 0.  ],\n",
       "       ...,\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
       "       [0.35, 0.  , 0.  , ..., 0.04, 0.  , 0.  ],\n",
       "       [0.  , 0.2 , 0.  , ..., 0.  , 0.51, 0.  ]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
