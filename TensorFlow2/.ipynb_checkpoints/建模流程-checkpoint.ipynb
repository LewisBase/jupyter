{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结构化数据建模流程范例\n",
    "\n",
    "## 准备数据\n",
    "\n",
    "## 定义模型\n",
    "\n",
    "使用Keras接口有以下3种方式构建模型：\n",
    "1. 使用Sequential按层顺序构建模型；\n",
    "2. 使用函数式API构建任意结构模型\n",
    "3. 继承Model基类构建自定义模型\n",
    "\n",
    "以下为Sequerntial按层顺序模型示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 20)                320       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 541\n",
      "Trainable params: 541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(20,activation='relu',input_shape=(15,)))\n",
    "model.add(layers.Dense(10,activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "训练模型通常有三种方法：\n",
    "1. 内置fit方法\n",
    "2. 内置train_on_batch方法\n",
    "3. 自定义训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二分类问题选择二元交叉熵损失函数\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['AUC'])\n",
    "\n",
    "histroy = model.fit(x_train,y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=30,\n",
    "                    validation_split=0.2 #分割一株分训练集用于验证\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(history,metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    val_metrics = history.history['val'+metric]\n",
    "    epochs = range(1,len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs,val_metrics,'ro-')\n",
    "    plt.title('Training and validation '+metric)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend(['train_'+metric, 'val_'+metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()\n",
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型\n",
    "\n",
    "Keras方式保存仅适合使用Python环境恢复模型，Tensorflow原生方式可以跨平台进行部署。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本数据建模流程范例\n",
    "\n",
    "## 准备数据\n",
    "\n",
    "tensorflow种完成文本数据预处理的常用方案又两种：\n",
    "1. 利用tf.keras.preprocessing种的Tokenizer词典构建工具和tf.kears.utils.Sequence构建文本数据生成器管道\n",
    "2. 使用tf.data.Dataset搭配keras.layers.experimental.preprocessing.TextVectorization预处理层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'the', b'and', b'a', b'of', b'to', b'is', b'in', b'it', b'i', b'this', b'that', b'was', b'as', b'for', b'with', b'movie', b'but', b'film', b'on', b'not', b'you', b'his', b'are', b'have', b'be', b'he', b'one', b'its', b'at', b'all', b'by', b'an', b'they', b'from', b'who', b'so', b'like', b'her', b'just', b'or', b'about', b'has', b'if', b'out', b'some', b'there', b'what', b'good', b'more', b'when', b'very', b'she', b'even', b'my', b'no', b'would', b'up', b'time', b'only', b'which', b'story', b'really', b'their', b'were', b'had', b'see', b'can', b'me', b'than', b'we', b'much', b'well', b'get', b'been', b'will', b'into', b'people', b'also', b'other', b'do', b'bad', b'because', b'great', b'first', b'how', b'him', b'most', b'dont', b'made', b'then', b'them', b'films', b'movies', b'way', b'make', b'could', b'too', b'any', b'after', b'characters']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,preprocessing,optimizers,losses,metrics\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import re,string\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, 'Not enough GPU hardware devices available'\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# 以上三行用于处理cuDNN无法卷积的错误\n",
    "\n",
    "\n",
    "train_data_path = './data/train_imdb'\n",
    "test_data_path = './data/test_imdb'\n",
    "\n",
    "MAX_WORDS = 10000 # 仅考虑最高频的10000词\n",
    "MAX_LEN = 200     # 每个样本保留200个词长度\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "# 构建管道\n",
    "def split_line(line):\n",
    "    arr = tf.strings.split(line,'\\t')\n",
    "    label = tf.expand_dims(tf.strings.to_number(arr[0],tf.int32),axis=0) # 从标量变为一维矢量\n",
    "    text = tf.expand_dims(arr[1],axis=0)                                 # 从标量变为一维矢量\n",
    "    return(text,label)\n",
    "\n",
    "ds_train_raw = tf.data.TextLineDataset(filenames=[train_data_path]) \\\n",
    "    .map(split_line,num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "    .shuffle(buffer_size=1000).batch(BATCH_SIZE) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test_raw = tf.data.TextLineDataset(filenames=[test_data_path]) \\\n",
    "    .map(split_line,num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "    .shuffle(buffer_size=1000).batch(BATCH_SIZE) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# 构建词典\n",
    "def clean_text(text):\n",
    "    lowercase = tf.strings.lower(text)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase,'<br />',' ')\n",
    "    cleaned_punctuation = tf.strings.regex_replace(stripped_html,\n",
    "                            '[%s]'%re.escape(string.punctuation),'')\n",
    "    return cleaned_punctuation\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=clean_text,\n",
    "    split = 'whitespace',\n",
    "    max_tokens=MAX_WORDS-1, # 有一个留给占位符\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN)\n",
    "\n",
    "ds_text = ds_train_raw.map(lambda text,label:text)\n",
    "vectorize_layer.adapt(ds_text)\n",
    "print(vectorize_layer.get_vocabulary()[0:100])\n",
    "\n",
    "# 单词编码\n",
    "ds_train = ds_train_raw.map(lambda text,label:(vectorize_layer(text),label)) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test_raw.map(lambda text,label:(vectorize_layer(text),label)) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  70000     \n",
      "_________________________________________________________________\n",
      "conv_1 (Conv1D)              multiple                  576       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv1D)              multiple                  4224      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  6145      \n",
      "=================================================================\n",
      "Total params: 80,945\n",
      "Trainable params: 80,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "class CnnModel(models.Model):\n",
    "    def __init__(self):\n",
    "        super(CnnModel, self).__init__()\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.embedding = layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN)\n",
    "        self.conv_1 = layers.Conv1D(16,kernel_size=5,name='conv_1',activation='relu')\n",
    "        self.pool = layers.MaxPool1D()\n",
    "        self.conv_2 = layers.Conv1D(128,kernel_size=2,name='conv_2',activation='relu')\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(1,activation='sigmoid')\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "    \n",
    "model = CnnModel()\n",
    "model.build(input_shape=(None,MAX_LEN))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "    \n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format('{}',m)) == 1:\n",
    "            return tf.strings.format('0{}',m)\n",
    "        else:\n",
    "            return tf.strings.format('{}',m)\n",
    "        \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                                timeformat(second)],separator=':')\n",
    "    tf.print('=========='*8,end='')\n",
    "    tf.print(timestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================14:17:30\n",
      "Epoch=1,Loss:0.140176237,Accuracy:0.94895,Valid Loss:0.43160513, Valid Accuaracy:0.8572\n",
      "\n",
      "================================================================================14:17:34\n",
      "Epoch=2,Loss:0.0940719694,Accuracy:0.9671,Valid Loss:0.528031051, Valid Accuaracy:0.86\n",
      "\n",
      "================================================================================14:17:39\n",
      "Epoch=3,Loss:0.0548578352,Accuracy:0.9817,Valid Loss:0.677602291, Valid Accuaracy:0.8564\n",
      "\n",
      "================================================================================14:17:44\n",
      "Epoch=4,Loss:0.0248573553,Accuracy:0.99235,Valid Loss:0.936459482, Valid Accuaracy:0.851\n",
      "\n",
      "================================================================================14:17:48\n",
      "Epoch=5,Loss:0.01326383,Accuracy:0.9956,Valid Loss:1.22467899, Valid Accuaracy:0.8448\n",
      "\n",
      "================================================================================14:17:53\n",
      "Epoch=6,Loss:0.0151005872,Accuracy:0.9948,Valid Loss:1.24381316, Valid Accuaracy:0.8454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Nadam()\n",
    "loss_func = losses.BinaryCrossentropy()\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.BinaryAccuracy(name='loss_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model,features,labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features,training=True)\n",
    "        loss = loss_func(labels,predictions)\n",
    "    gradients = tape.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "    \n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels,predictions)\n",
    "    \n",
    "@tf.function\n",
    "def valid_step(model,features,labels):\n",
    "    predictions = model(features,training=False)\n",
    "    batch_loss = loss_func(labels,predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels,predictions)\n",
    "    \n",
    "def train_model(model,ds_train,ds_valid,epochs):\n",
    "    for epoch in tf.range(1,epochs+1):\n",
    "        for features, labels in ds_train:\n",
    "            train_step(model,features,labels)\n",
    "            \n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model,features,labels)\n",
    "            \n",
    "        # 此处logs模板需要根据metric具体情况修改\n",
    "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{}, Valid Accuaracy:{}'\n",
    "        \n",
    "        if epoch%1 == 0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "                (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print('')\n",
    "            \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "        \n",
    "train_model(model,ds_train,ds_test,epochs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估模型\n",
    "通过自定义训练循环的模型没有经过编译，无法直接使用model.evaluate(ds_valid)方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,ds_valid):\n",
    "    for features, labels in ds_valid:\n",
    "        valid_step(model,features,labels)\n",
    "    logs = 'Valid Loss:{},Valid Accuracy:{}'\n",
    "    tf.print(tf.strings.format(logs,(valid_loss.result(),valid_metric.result())))\n",
    "    \n",
    "    valid_loss.reset_states()\n",
    "    train_metric.reset_states()\n",
    "    valid_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss:1.24381328,Valid Accuracy:0.8454\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model,ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用模型\n",
    "可以使用以下方法：\n",
    "* model.predict(ds_test)\n",
    "* model(x_test)\n",
    "* model.call(x_test)\n",
    "* model.predict_on_batch(x_test)\n",
    "\n",
    "推荐优先使用model.predict(ds_test)方法，既可以对Dataset，设可以对Tensor使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0103064e-09],\n",
       "       [5.9294913e-10],\n",
       "       [4.9843590e-10],\n",
       "       ...,\n",
       "       [9.7289586e-01],\n",
       "       [2.3512270e-10],\n",
       "       [1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5.3290337e-01]\n",
      " [1.7145459e-06]\n",
      " [1.8435133e-08]\n",
      " [1.0000000e+00]\n",
      " [9.9999869e-01]\n",
      " [9.9999976e-01]\n",
      " [2.2574291e-01]\n",
      " [5.9972596e-01]\n",
      " [8.4904391e-01]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0307520e-15]\n",
      " [3.6671326e-11]\n",
      " [8.5513610e-01]\n",
      " [1.0000000e+00]\n",
      " [5.2997330e-14]\n",
      " [4.7708176e-02]\n",
      " [2.1040199e-13]\n",
      " [6.0539908e-20]\n",
      " [9.0309720e-07]], shape=(20, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[5.3290337e-01]\n",
      " [1.7145459e-06]\n",
      " [1.8435133e-08]\n",
      " [1.0000000e+00]\n",
      " [9.9999869e-01]\n",
      " [9.9999976e-01]\n",
      " [2.2574291e-01]\n",
      " [5.9972596e-01]\n",
      " [8.4904391e-01]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0307520e-15]\n",
      " [3.6671326e-11]\n",
      " [8.5513610e-01]\n",
      " [1.0000000e+00]\n",
      " [5.2997330e-14]\n",
      " [4.7708176e-02]\n",
      " [2.1040199e-13]\n",
      " [6.0539908e-20]\n",
      " [9.0309720e-07]], shape=(20, 1), dtype=float32)\n",
      "[[5.3290337e-01]\n",
      " [1.7145459e-06]\n",
      " [1.8435133e-08]\n",
      " [1.0000000e+00]\n",
      " [9.9999869e-01]\n",
      " [9.9999976e-01]\n",
      " [2.2574291e-01]\n",
      " [5.9972596e-01]\n",
      " [8.4904391e-01]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0307520e-15]\n",
      " [3.6671326e-11]\n",
      " [8.5513610e-01]\n",
      " [1.0000000e+00]\n",
      " [5.2997330e-14]\n",
      " [4.7708176e-02]\n",
      " [2.1040199e-13]\n",
      " [6.0539908e-20]\n",
      " [9.0309720e-07]]\n"
     ]
    }
   ],
   "source": [
    "for x_test,_ in ds_test.take(1):\n",
    "    print(model(x_test))\n",
    "    # 一下新方法等价\n",
    "    print(model.call(x_test))\n",
    "    print(model.predict_on_batch(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/tf_model_savemodel_imbd/assets\n",
      "export saved model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.9904126e-01],\n",
       "       [1.6086851e-13],\n",
       "       [1.0053123e-12],\n",
       "       ...,\n",
       "       [7.4865986e-03],\n",
       "       [9.9999082e-01],\n",
       "       [1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('./data/tf_model_savemodel_imbd',save_format='tf')\n",
    "print('export saved model')\n",
    "\n",
    "model_loaded = tf.keras.models.load_model('./data/tf_model_savemodel_imbd',compile=False)\n",
    "# 加入compile=False可消除模型中没有训练配置的警告\n",
    "model_loaded.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
