{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用机器学习与情感分析\n",
    "\n",
    "本章将主要涵盖下述几个方面\n",
    "\n",
    "* 清洗和准备文本数据\n",
    "* 根据文本数据建立特征向量\n",
    "* 训练机器学习模型来区分正面或者负面评论\n",
    "* 用基于外存的学习方法来处理大型文本数据集\n",
    "* 根据文档推断主题进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为文本处理预备好IMDb电影评论数据\n",
    "\n",
    "### 获取电影评论数据集\n",
    "\n",
    "### 把电影评论数据预处理成更方便格式的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:08\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# change the 'basepath' to the directory of the\n",
    "# unzipped movie dataset\n",
    "\n",
    "basepath = 'aclImdb'\n",
    "\n",
    "labels = {'pos':1,'neg':0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test','train'):\n",
    "    for l in ('pos','neg'):\n",
    "        path = os.path.join(basepath,s,l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path,file),\n",
    "                     'r',encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt,labels[l]]],\n",
    "                           ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review','sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码首先初始化新的进度条对象pbar，并定义迭代次数为50000，这是要读入的文件数量。使用嵌套的for循环，遍历aclImbd主目录下的train和test子目录，并从子目录pos和neg下读入单个文本文件，这两个目录连同整数类标签（1=正面和0=负面）最终将会被映射到pandas的DataFrame对象df上。\n",
    "\n",
    "因为数据集中的分类标签已经排过序，所以现在可以调用np.random子模块的permutation函数对DataFrame洗牌，这对后期将数据集分裂成训练集和测试集很有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>In the New Year's Eve, the tuberculous sister ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Mr. Kennedy should stop ExPeRiMeNtIng with bad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>This is a terrible production of Bartleby, tho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In the New Year's Eve, the tuberculous sister ...          1\n",
       "1  Mr. Kennedy should stop ExPeRiMeNtIng with bad...          0\n",
       "2  This is a terrible production of Bartleby, tho...          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv',encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词袋模型介绍\n",
    "\n",
    "词袋模型背后的逻辑非常简单，可以概括如下：\n",
    "\n",
    "1. 从整个文档中创建一个基于独立令牌（例如单词）的词汇表。\n",
    "2. 为每个文档构建一个特征向量，其中包含每个词在特定文档中出现的频率。\n",
    "\n",
    "由于么个文档的独立单词只代表了词袋词汇表中所有单词的一小部分，所以特征向量主要由零组成，因此称之为稀疏向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把此转换成特征向量\n",
    "\n",
    "可以用scikit-learn实现的CountVectorizer类根据单词在各文件中出现的频率构建词袋模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shing, the weather is sweet, and one and one is two'\n",
    "])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用CountVectorizer的fit_transform方法处理构建词袋模型的词汇表，并且把以下三个句子转化为稀疏特征向量。\n",
    "\n",
    "现在打印出词汇表的内容，以便更好地理解其中所包含地概念"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'sun': 5, 'is': 1, 'shining': 4, 'weather': 9, 'sweet': 6, 'shing': 3, 'and': 0, 'one': 2, 'two': 8}\n",
      "[[0 1 0 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 0 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过词频逆反文档频率评估单词相关性\n",
    "\n",
    "__词频逆反文档频率(td-idf)__，用于减少特征向量中频繁出现地词。tf-dif可以定义为词频与逆反文档频率地乘积：\n",
    "\n",
    "$$tf-idf(t,d)=tf(t,d)\\times idf(t,d)$$\n",
    "\n",
    "tf(t,d)为前一节引入的词频，idf(t,d)为逆反文档频率，起计算过程如下：\n",
    "\n",
    "$$idf(t,d)=\\log \\frac{n_d}{1+df(d,t)}$$\n",
    "\n",
    "$n_d$为文档总数，$df(d,t)$为含有单词t的文档数量。请注意，为分母添加常数1为可选，目的在于所有训练样本中出现地单词赋予非零值，用对于来确保低文档频率地权重不会过大。\n",
    "\n",
    "scikit-learn实现了另外一个转换器TfidfTransformer类，它以来自于CountVectorizer类，以原始词频为输入，然后转换为tf-idfs格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.39 0.   0.   0.66 0.5  0.   0.39 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.44 0.5  0.25 0.   0.19 0.19 0.29 0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True,\n",
    "                         norm='l2',\n",
    "                         smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs))\n",
    "      .toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
